\section{FourierTransform: Fast Fourier Transforms}
[Warren Smith prepared the analysis and modules.]

This is an {\em in place} FFT routine with array length:  $N = 2^n$.
\begin{equation}
 a_{output,k} = \sum_{m=0}^{N-1} a_{input,m} 
        e^{(2 \pi i \mbox{direction} m k/N)}
\end{equation}

where direction $= \pm 1$.
You have to do any scaling (by $1/N$ or $1/\sqrt{N}$) or zeroing of a[] by
yourself. 

Uses $2N \ln N + 2 N + O(\ln N)$ [real] multiplications, 
and $ 3N \ln N + 2 N + O(\ln N)$ [real] additions.
(And hopefully the compiler optimizes the subscripting in the inner loop so
that only $2 N \ln N$ total subscripting ops are needed.)

The test routine shown yields
error after 1 forward and one backward use
at least 3 decimal places better than
slow FT has after only one use!

The basic FFT idea (for N a power of 2) is from J.Cooley \& J.Tukey:
An algorithm for the Machine Calculation of Complex Fourier Series,
MOC 19 (1965) 297-301. (Knuth, Henrici, and Dahlquist/Bjorck also discuss
the FFT.) The basic idea is that to calculate the DFT
\begin{eqnarray}
    A[k]  & = & \sum_{j=0}^{N-1}a_j*W^{kj}  \, \mbox{for k=0..N-1}\\
    N & = & \mbox{a power of 2,}\, N>1\\
    W & = & \mbox{a principal Nth root of unity}\\
   \mbox{e.g., } W & = & e^{2 i \pi/N}
\end{eqnarray}
We may write
\begin{eqnarray}
    A_k & = & g_k + h_k * W^k \, \mbox{ where}\\
    g_k & = & \sum_{0<=2j<N} a_{2j}   * (W^2)^{k j}\\
    h_k & = & \sum_{0<=2j<N} a_{2j+1} * (W^2)^{k j}
\end{eqnarray}
are FFTs of half the size (on the even and odd indexed a[]'s, respectively)
note that $g_k$ and $h_k$ are periodic in k with period $N/2$, 
since $W^{N/2} = -1$,
so that if $T(N)=\mbox{time to calculate FFT of size N}$, then 
$T(N)=2T(N/2)+O(N)$ and so $T(N)=O(N \ln N)$. The inverse transform is:
\begin{equation}
    a_k  =  \mbox{Ninverse}  \sum_{j=0}^{N-1} a_j 
              \mbox{Winverse}^{k j} \mbox{  for k=0..N-1}
\end{equation}
also an FFT and is calculable by the same method. (Ninverse*N=1. Winverse*W=1.)
Cooley-Tukey also works for general highly composite N.
A short and elegant implementation is:
[Warren E.Ferguson: A simple derivation of glassman's general N FFT,
Computers \& Math. with Applics. 8 (1982) 401-411].

For an efficient implementation of the FFT, further refinements are desired.
These include: 
\begin{enumerate}
\item Removal of the recursion by "reverse binary permuting" the
original data; 
\item "In place" implementation with no extra storage requirement;
\item Calculation of the $W^k$'s (k=0,1..) by efficient and stable recurrences
(thus avoiding the need for transcendental functions); 
\item Possibly use "fast complex multiplication"; 
\item In specific but common applications,
some further savings may be possible. Thus when doing a fast convolution
of two arrays of N reals using an FFT, the two FFTs may be 
accomplished by ONE N point complex FFT, 
followed by linear time uncombining/termwise
multiplication step, followed by a N/2 point reverse FFT. 
The reverse binary
permuting stages may be avoided completely. 
\end{enumerate}

The basic symmetry here is that
the FFT of a real array is "conjugate even", e.g.
\begin{equation}
    A_k = \mbox{CompConjugate}(A_{N-k}) \mbox{ if a[0..n-1] is real}
\end{equation}

R.C.Singleton [e.g. see his algorithm for fast circular convolutions,
CACM 12,3 (March 1969) 179; his FFT2 algorithm, CACM 11 (Nov 1968) 773-779;
and his article CACM 10 (Oct 1967) 647-654] has suggested a second 
difference
method [which keeps the real and imaginary parts of exp(ick) uncoupled] 
for evaluating exp(ick):    
\begin{eqnarray}
    e^{ic(k+1)} & = & e^{ick} + I_{k+1}\\
    I_{k+1} & = & -4 \sin^2(c/2) e^{ikc} + I_k\\   
    I_0 & = & 2 i \sin(c/2) e^{-ic/2}
\end{eqnarray}
which is both faster and experimentally far more stable (typically
yielding 500 times smaller error for 128 point transform+inverse transform)
than the straightforward "multiply by exp(ic)" method. (It's more stable
because of the small multiplier $-4 \sin^2(c/2)$, as opposed to 1.)

However a better idea which I recently thought of is to use this recurrence:
\begin{equation}
    e^{ic(k+1)} = e^{ic(k-1)} + 2 i \sin(c) e^{ick}
\end{equation}
which requires only 2*,2+ per complex exponential (Singleton is 2*,4+;
naive method is 4*,2+ and isn't stable) and is also stable since it involves
the small multiplier $2 i \sin(c)$. (In fact, this method runs faster than
Singleton, is easier to program, and even yielded slightly better accuracy
in the test program below, too!) Therefore this modification of Singleton
is the method I've used.

Incidentally, Press et al. in their NR book use the Singleton algorithm
but neglect to credit Singleton!

By a trivial modification of my code, one could generate the sines and
cosines by repeated application of the bisection identities:
\begin{eqnarray}
    \cos(t/2) & = & \sqrt{0.5 (1.0 + \cos(t))}\\
    \sin(t/2) & = & 0.5 \sin(t)/\cos(t/2)
\end{eqnarray}
starting from the values with $t=\pi$ and $t=\pi/2$ as special cases, thus
avoiding trig subroutine calls entirely. 
OK, I've now done this; now using precomputed table.

The time savings introduced by either of these is small, however.
Finally, "radix 8" transforms are experimentally the most efficient,
typically 20\% faster than "radix 2" routines like this one, although far
more complicated; and anyway I suspect the advantage is $<20\%$ 
in the modern cache-memory system world, since I suspect the radix 2 algorithm
has better cache locality. However, that has not been tested.

Also you could take advantage of 1's and 0's to save a little time at
the expense of considerably more space.

Another idea which I have chosen not to implement is the fact that two
complex numbers may be multiplied in 3 real multiplications:
Thus  $E+iF = (a+bi)*(c+di)$  may be accomplished via the instructions
\begin{eqnarray}
  bpa & = & b+a; \\
  bma & = & b-a; \\
  E & = & a*(c+d);\\ 
  F & = & bma*c+E; \\
  E & -= & bpa*d;
\end{eqnarray}
and if bpa and bma are precomputed, this is a 3*,3+ method for a complex
multiplication. This idea may be used to reduce (?) the box score from
the present  $2N \ln N+2N+O(\ln N)$ mults, 
$3N \ln N+2N+O(\ln N)$ adds  to
$1.5N \ln N+2N+O(\ln N)$ mults  and  $3.5N \ln N+4N+O(\ln N)$ adds. 
If a floating point
multiplication is M times slower than a floating point addition,
this idea pays iff $ln N>4/(M-1)$. On PDP-11/44 C, however, rough timing
has shown that $M=1.08$ (but with considerable standard deviation.
It does
about $5*10^4$ additions/sec.) so this idea is not worth it unless N is
enormous.

Some similar, but worse, ideas have been suggested by Buneman:
\begin{verbatim}
 If c=cos(m),s=sin(m), then precompute  
    t1 = (1-c)/s = s/(1+c)= tan(m/2)  and
    t2 = (1+s)/c = c/(1-s). 
\end{verbatim}
Then $X+iY = (a+bi)*(c+is)$ may be found in 3*,3+ by:
\begin{verbatim}
  if(|t1|<|t2|) then
     X = a-t1*b; Y = b+s*X; X -= t1*Y;
  else
     X = b+t2*a; Y = c*X-a; X -= t2*Y;
\end{verbatim}
tans may be updated by  
$\tan(x+y)-\tan(x-y) = 2*\tan(y)/(1-(\tan(x)*\tan(y))^2)$,
but the extra overhead seems not to be worth it].

There is also a symmetric 3*,5+ (4+ with precomputation) formula for  
$E+iF = (a+bi)*(c+di)$:
\begin{eqnarray}
  E & = & a*c-b*d\\
  F & = & (a+b)*(c+d)-a*c-b*d
\end{eqnarray}

Other FFT algorithms: Winograd has shown how to design FFTs with N prime
(as opposed to the Cooley-Tukey approach which works for N highly composite)
that run in $O(N \ln N)$ time and even with only O(N) multiplications;
the latter figure is optimal. [S.Winograd: Math. of Comput. 32 (1978) 175-179;
Advs in Math 32 (1979) 83-117].

Winograd's approach is based on a theorem that allows him (by a permutation
of the original and transformed variables) to express FFTs for N prime
in terms of a circular convolution of N-1 elements, plus some additions.
He then shows how circular convolutions of k elements (for certain small k)
may be computed in a small number of arithmetic operations, (for k=2..6,
the number of multiplications Winograd uses is 2,4,5,10,8; for k prime,
Winograd shows that a (2k-2)* scheme for CC(k) always exists) and further, how
CC(n1) and CC(n2) algorithms may be composed to make a CC(n1*n2) algorithm, IF
n1 and n2 are relatively prime, that uses mult(n1)*mult(n2) multiplications.
Also, he shows how FFT(n1*n2) may be computed via FFT(n1) and FFT(n2)
in mult(n1)*mult(n2) multiplications, IF n1,n2 relatively prime, and
also gives methods for FFT(prime power). He gives two appendices
containing optimized CC(2..6) and FFT(2..9) algorithms. Winograd's methods
don't appear suitable for general N, but if N is specified in advance, they
make it possible to do considerable fine tuning at the expense of
large algorithm complexity.

Meanwhile, Nussbaumer [H.J.Nussbaumer: Fast Polynomial Transform algorithms
for digital convolutions, IEEE Transactions on Audio, Speech, Signal Processing
28,2 (April 1980) 205-215 (this article has many references to other FFT
schemes); see also Knuth 2: 503, 652-653] has found another way to do circular
convolutions of arrays of (N=a power of 2) reals without any NTTs, FFTs, trig,
or complex numbers. His approach is based on viewing circular convolutions
as polynomial multiplications modulo certain simple polynomials, factoring the
modular polynomials, divide and conquer, chinese remainder thm.
His approach uses roughly NlgN *,
$N \ln N \ln \ln N$ +, is fairly complicated to program, and requires extra space.

[Nussbaumer \& Quandalle: IBM JResDev 22 (1978) 134-144] show how some
particularly efficient CC and FFT schemes for N in the range 10-3000
may be constructed; their approach is based on some novel ways to combine
efficient small CC schemes that is rather like the NTT (Number theoretic
transform) only in rings of polynomials rather than in the integers.

However even the best known arithmetic op count methods only improve on my
method by perhaps 30\%, and at the cost of considerable complexity.
C.H.Papadimitriou [Optimality of the FFT, JACM 26 (1979) 95-102 and its refs]
has shown that in some models of computation $O(N\ln N)$ is
optimal for the FFT, while Patterson et al have shown a lower
bound of $O(N\ln N/\ln \ln N)$ for integer multiplication on multitape
Turing machines, see Knuth 2.

FFTs in a finite field (if the ring ZmodK, called "number theoretic
transforms") are discussed in Aho,Hopcoft,Ullman: The Design and Analysis
of Computer algorithms, Addison-Wesley 1974. They recommend using W=2,
N=a power of 2, do all arithmetic in the ring of integers 
modulo $2^{N/2}+1$
(in which W is an Nth root of unity, and in which the convolution theorem
\begin{equation}
    C_i=A_iB_i  \Longleftrightarrow   
         c_i = \sum_{j=0}{N-1} a_i*b_{i-j \bmod N}
\end{equation}
[Which makes possible the calculation of discrete convolutions in $N\ln N$ time]
still holds). [See also R.Agrawal\&C.Burrus: NTTs to implement fast digital
convolutions, ProcIEEE 63 (1975) 550; articles by H.Nussbaumer on "Fermatrix"
and "Mersenne" transforms, IBMJR\&D 21 (1976) 282 and 498.]

These FFFFTs are of use in applications where it is desirable
to completely eliminate roundoff error and floating point operations,
e.g. all-integer convolutions. However, as you can see, NTTs have severe
word length and transform length limitations; the need for high precision
modular arithmetic can be a major stumbling block. On the other hand,
multiplications by W=2 are easy, while modular arithmetic modulo a
Fermatrix number is not that hard. Thus using N=16, modulo 65537 arithmetic,
W=2 [left shift and modulo], Winverse=32769 [right shift; modular
addition correction if inexact], and all numbers in 0..89 allows
computation of CC(16) in 16*, many bit shifts and additions.

Rabiner,Schafer,Rader: The Chirp-Z transform and its Applications,
BSTJ 48,3 (1969) 1249-1292 show how DFTs (for any N) may be calculated in
$N\ln N$ time by using fast convolutions; the method also works for an extension
of FFTs (to W=any complex number, not just the principal Nth root of unity):
\begin{equation}
    A_k  =  \sum_{j=0}^{N-1}a_j*W^{k*j}  \mbox{ for k=0..N-1}
\end{equation}
may be calculated in $N\ln N$ time by a fast convolution by the "Chirp-Z
transform" identity
\begin{equation}
    A_k  = W^{k*k/2} \sum_{j=0}^{N-1} W^{-((j-k)^2)/2} W^{j*j/2}b_j
\end{equation}

Aho,Stieglitz,Ullman: Evaluating Polynomials at fixed sets of points,
SIAMJComp 4,4 (Dec 1975) 533-539, demonstrate that a polynomial and all its
derivatives at one point (or equivalently, an origin shift of an Nth
degree polynomial) may be calculated in $N\ln N$ time by a fast convolution
via the (binomial theorem) identity
\begin{eqnarray}
    \sum_{j=0}^{N-1} c_j*(x+q)^j  & = & \sum_{r=0}^{N-1} x^r * d_r/r!
  \mbox {  where}\\
    d_r & = & \sum_{j=r}^{N-1} c_j*j! * q^{j-r}/(j-r)!
\end{eqnarray}

Some other applications of FFTs are:

Fast multiplication and division of N digit integers may be done in
(roughly) $N\ln N$ time by using fast convolutions followed by a carry step.
(See Aho-Hopcoft-Ullman: Design and Analysis of Computer algorithms, for
further discussion.)

Base conversion of an N digit number may be done in $N(\ln N)^2$ time by
divide and conquer (convert the left and right half of the number recursively,
then do a fast multiplication and addition to combine them).

Fast polynomial multiplication and division by fast convolutions in $N\ln N$
time are also discussed in AHU. (This may also be done for Chebyshev
series...) Given the N roots of a polynomial, you can find its
coefficients (as Chebyshev or as regular) in $N(\ln N)^2$ time by fast
polynomial multiplications on a binary tree. On the other hand,
you can perform a "root squaring" transformation on a polynomial
in (Chebyshev or power form)
\begin{equation}
   P(y) = -P(x)*P(-x)  \mbox{ where }y = x^2
\end{equation}
in $N\ln N$ time by a fast multiplication, or alternatively can implement
a Henrici-Gargantini or Korsak-Pease (or other) simultaneous all root iteration
step, in $N(\ln N)^2$ time by a fast multipoint evaluator, see below.

All shifted correlations of vectors (and/or autocorrelations) may be
calculated in $N\ln N$ time by fast convolutions; this has application in
signal processing, 1D pattern recognition, Electrical engineering.

Fast polynomial multiplication/division/remaindering and a divide and
conquering of the Lagrange interpolation formula may be used to do fast
Nth degree polynomial interpolation and N-point evaluation, as was
shown by Borodin\&Moenck [JCompSystSci 1974]. I have extended B\&M's
results to Chebyshev polynomials and less successfully to other
polynomials.

Fast algorithms exist for power-series to continued fraction interconversion;
these may also be generalized to Chebyshev series.

Fast polynomial evaluation/interpolation at special point sets
(e.g. $Z^k$ for some Z) may be accomplished in $N\ln N$ time by the Chirp-Z and
FF transforms; this also carries over to Chebyshev. Thus fast Taylor
and Chebyshev series calculations.

Fast composition of Taylor series -
$O((N\ln N)^{3/2})$ is also possible, as was discovered by Brent\&Kung,
via a "block Horner" approach. This may also be extended to Chebyshev.

Fast Elliptic linear PDE solvers (by finite differneces or spectrally):
there are many schemes based on FFTs that run in $N\ln N$ time, N=size of
output.

A complete list of FFT applications is far too huge to discuss here...

