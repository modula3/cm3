\section{Concepts}

\subsection{Overview}
Numerical Analysis takes place within the context of a
discrete computing machine with finite register sizes.  To
be specific, we can use the Intel 386/387 as the least
common denominator for Modula-3.  Further, Modula-3 is
currently implemented with a 32-bit Word and IEEE 32-bit and
64-bit floating points numbers, which happen to mirror the
386/387 hardware.  A large part of numerical analysis is
tuning algorithms so that they are not tripped up by the
finite limitations of the hardware.

First we must understand the number representations.  Then
we can discuss the typical errors which can arise.  Next we
will discuss ways around them.  Finally, we will layout an
approach to numerical analysis routines.

\subsection{Arithmetic Logic Unit (ALU)}
See Craw87, pg 18.

An ALU is the integer heart of a typical CPU.  It handles
raw machine word operations like:
\begin{itemize}
   \item  rotate
   \item  shift
   \item  bitset and reset
   \item  compare
   \item  test for sign
   \item  test for carry
\end{itemize}

It also does "mathematical" cardinal and integer operations
like:
\begin{itemize}
   \item  add
   \item  sub
   \item  mul
   \item  div
\end{itemize}
     
The ALU's 32 bits are typically drawn as 0..31 bits, with
the most significant figure on the left.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline
    31 & 30 & 29 & $\cdots$ & 2 & 1 & 0 \\ \hline
\end{tabular}    
\end{center}

The raw values in the register are typically as given as
bitstrings of "0" and "1".  Modula-3 conveniently allows
this to be shown as a literal, preceded by the radix marker
"2\_".  You don't have to show the 0's to the left of the
most significant 1, but it doesn't hurt.  Often for
formatting, it is a good idea to left-pad with leading 0's
to a given width:

\begin{tt} \begin{verbatim}
     2_1010101010101010101010101010101 (*all 32 bits used*)
     2_10001010 (*just lowest 8 bits used*)
     2_00001011 (*just lowest 4 bits are used*)
\end{verbatim} \end{tt}

If you do nothing special, these literals will be fitted
into a full 32-bit INTEGER when they are assigned to a
variable.  But by defining types (and variables) with fewer
bits, we could save space:

\begin{tt} \begin{verbatim}
     RECORD
       i:=2_1010;                     (*32-bit INTEGER*)
       j:BITS  4 FOR [0..15]:=2_1010; (* 4-bit CARDINAL*)
       k:BITS 21 FOR [0..1000000];    (* 21-bit CARDINAL*)
       l:BITS  7 FOR [-5..50];        (* 7-bit INTEGER*)
     END;
\end{verbatim} \end{tt}

This makes sense in a packed record, but probably not
elsewhere.  For numerical analysis, Integer.i3 provides integers
and cardinals of 8, 16, 32 bits.

Completely independent of the choice of bit lengths is the
choice of radix for representing literals.  While binary
literals help clarify some numerical code, other radixes
might help elsewhere.  Unlike many languages, Modula-3
allows radixes from 2..16, e.g.,
\begin{tt} \begin{verbatim}
     8_03470
     16_12AF1
     3_012
     11_12AB3
\end{verbatim} \end{tt}

While raw bits can be used to implement many data types,
here we are only concerned with numbers.  Unsigned numbers
build up as binary numbers, for $0 \dots (2^{32} - 1)$.  Signed numbers
are represented in 2's complement form, for $-2^{31} \dots (2^{31} -
1)$.  See Craw87, pg 7 for details.

The critical thing to know about 2's complement is that the
32nd bit is used to indicate sign.  Thus, it is not
available for use in building number magnitudes.  Modula-3
reflects this by making {\tt CARDINAL}  go $0 \dots (2^{31} -1)$, 
making it a subtype of {\tt INTEGER}.  What about unsigned numbers from
$2^{31} \dots (2^{32}-1)$?  You must use Word.T for those, and use the
Word module's operations.  While these look like function
calls (and thus slow), they are really compiled inline as
the obvious assembler statisticements.

What can go wrong?  Integer matrix is a sure thing, so long as
you don't overflow (e.g., go beyond $2^{31}-1$ for {\tt CARDINAL}) or
underflow (e.g., go below $0$ for {\tt CARDINAL}s or $-2^{32}$ for
{\tt INTEGER}s).  Modula-3 traps these conditions, but you should
plan your algorithm to avoid the conditions.  Use multi-
precision packages if you need more resolution.

For m3na, we will use {\tt CARDINAL}s when we mean counting
numbers (such as for array indices), and {\tt INTEGER}s when we
mean quantities which might be negative.

\subsection{Floating Point Unit (FPU)}
See Craw87, pp 20-30.

The 387 FPU provides IEEE 32-bit, 64-bit, and 80-bit reals.
These are structured as a sign bit, an exponent, and a
significand (also known as mantissa):
\begin{verbatim}
     32-bit
          sign        =1 bit
          exponent    =8 bits
          significand =23 bits (really 24 due to hidden 1)
          range       =+/- 3.39E38
          precision   =+/-1.18E-38
     
     64-bit
          sign        =1 bit
          exponent    =11 bits
          significand =52 (really 53 due to hidden 1)
          range       =+/- 1.08E308
          precision   =+/- 2.23E-308
     
     80-bit
          sign        =1 bit
          exponent    =15 bits
          significand =64 bits
          range       =+/- 1.19E4932
          precision   =+/- 3.36E-4932
\end{verbatim}

There are several problems in using these finite
representations for real numbers.
\begin{description}
\item[Out of Range]
     Suppose you want to represent one googol (1.0E100).  It
     just can't be done in the 32-bit format.  That may seem
     fanciful for practical problems, but intermediate
     values (e.g., sums of squares) might go beyond the 32-
     bit limits.

\item[Representation Error]
     Many decimal numbers do not convert exactly to/from
     binary.  You have to use a representation whose
     precision is sufficient that this error is not a
     problem.

\item[Truncation Error]
     Suppose you add two numbers in 32-bit format: $a=1.0e6$ and $b=1.00123$.
     
     To get a valid addition, we would need to get both a
     and b into the significand, both with the same
     exponent: $(1.0+0.00000100123)*10^6 = 1.00000100123e6$.
     There aren't enough bits in the significand to
     represent this.  It gets truncated to: $1.000001e6$.
     
     What we need to know is how small one number can be
     relative to another before part of the smaller gets
     lopped off.  This small amount is usually known as
     $\epsilon$ (epsilon).  The value is differnet for various
     representations, giving EPS32, EPS64, and EPS80.
     These are calculated as:
     \begin{verbatim}
          max = 2^sigbits
          rel prec = 1/max = 2^(-sigbits)=~ sigbits * ln(2)/ln(10)
          
          32-bit: about 1.0E-7 = ESP32
          64-bit: about 1.0E-17= EPS64
          80-bit: about 1.0E-24= EPS80
     \end{verbatim}
     
     As a consequence, there is no point in trying to tune
     an iteration solution beyond a certain point, e.g.:
     \begin{tt} \begin{verbatim}
         IF delta < value*EPS32 THEN
            EXIT; (*can't do any better*)
         ELSE
            value:=value+delta; (*room for improvement*)
         END;
     \end{verbatim} \end{tt}
     
     Truncation can happen as in the above example with
     addition and subtraction.  It can also happen with
     multiplication.  This is particularly true for
     intermediate values (such as squares).  It is worth
     your while to arrange to normalize the calculation
     toward 1.0 before doing multiplications.  Or use a
     longer real format.

\item[Roundoff Error]
     If you are in danger of truncating, you may round up or
     down.  If the rounding is not done right, it will
     introduce a bias into the calculation.
     
     Fortunately, Modula-3 reals are specified as IEEE
     reals, which include extra digits to assure proper
     rounding.
\end{description}

\subsection{32-bit vs 64-bit}
Is REAL32 good enough?  NR92, pg 25, suggests it is, and in
fact uses 32-bit reals for its printed algorithms.

However, Gems90 uses 64-bit reals, despite operating in a
speed-driven environment with low end-result resolution
(viewable by human eye).  Hopkins says:
\begin{quote}
     "Generally, if the real arithmetic has a mantissa of
     less than about 30 bits, then a double precision
     version of the library is produced.  In fact the double
     precision [64-bit] version is by far the most common,
     since most architectures which mimic IBM floating-point
     hardware and the IEEE floating-point standard all
     define the mantissa of real variables to be around 24
     bits.  This accuracy is not sufficient for reliable
     numerical computation."  Hopk88, pg 34.
\end{quote}

Further, the Modula-3 Math library is 64-bit.  Thus it
appears the choice is REAL64.

\subsection{Timing}
Each machine will have different timing for its hardware
ALU and FPU operations, and each compiler will have
different approaches for calling upon these operations (thus
further diversifying timing).  However, it is probable that
for the class of problems we are addressing, we can
determine the rough relative timing cost for operations.

Craw87 provides timing data in the appendices.  While every
combination of source/dest is slightly different, we can use
the "best-case" for the register-to-memory operation as the
exemplar.  If we normalize the cycles to let integer add (32
bit) be "1", and round to integers, the ratios are:
\begin{verbatim}
     iadd32         1
     imul32         5
     idiv32         6
     fadd32         3
     fadd64         4
     fmul32         4
     fmul64         5
     fdiv32         12
     fdiv64         13
\end{verbatim}

In other words, integers are a lot faster than floating-
point, and divide is the killer.  Interestingly, 64-bit is
not much worse than 32-bit.  Thus our choice of 64-bit as
the basic format is not all that extravagant.


\subsection{Development Approach}
Here is our approach to developing routines:
\begin{enumerate}
     \item Pick an interesting topic.  If you aren't
     interested, you won't stick with it long enough to do a
     good job.
     
     \item Gather relevant sources (e.g., texts, code
     libraries, journal articles).  Determine an effective
     user interface.  Determine test cases from known-good
     sources such as matrix packages or handbooks.
     
     \item Study the problem until you can explain the standard
     implementation(s) line by line.  You may need to work
     backwards and forwards from the mathematical formulas
     to the code.  You typically need to work out simple
     (but not too simple) examples, e.g., 4x4 matrix.
     
     \item Study the iterative operations for truncation
     conditions.  Do you need to renormalize the data toward
     1.0?  Do EPS checks?  Use a longer real format?
     
     \item Implement your version of the algorithm.
     \begin{enumerate}
          \item Test it for basic correctness against known
          good values.  Don't worry about being off by a few
          parts in a thousand.  At this stage you should be
          concerned with wildly wrong results.  If it made
          sense on paper, and you got it to compile, then
          chances are it is a fencepost error (off-by-one)
          in the iteration conditions.  Debug as needed.
          
          \item Test for full accuracy against known-good
          values.  Does your algorithm fall apart with big
          or little numbers?  Is it erratic?  Check for
          truncation errors and more fencepost errors.
          Debug as needed.
          
          \item Test for speed against published algorithms.
          Use the Time module and multi-second runs.
       \end{enumerate}
\end{enumerate}

When it comes time to worry about truncation errors and
timing, look for normalization toward the center of the
range.  The classic is the hypotenuse of a right triangle:
\begin{verbatim}
          hyp:=sqrt(a^2 + b^2); (*where a>b*)
          (*a^2 is in danger of producing an overflow
            although the final result
            is perfectly in the representable range.*)
          
          (*Instead, do:*)
          r:=(b/a);
          hyp:=abs(a)*sqrt(1.0+r*r);
\end{verbatim}

When it comes time to worry about speed, consider:
\begin{enumerate}
     \item Precalculate anything you can:
     \begin{itemize}
          \item Compile-time constant propagation
          \item Runtime initialization
          \item Calltime initialization
          \item Outer loop instead of inner loop
          \item Common subexpressions
      \end{itemize}

     \item Unroll loops, especially if you know the algorithm will
     only need a few iterations.
\end{enumerate}
     
Do you really need to do these things?  Modern compilers
know a great deal about such local optimizations.  If you
can do it and the code becomes more readable and more
maintainable, then do so.  But don't screw up clean code for
a few percent speedup.  You will still be debugging when a
faster computer shows up on your desk.  Instead use your
time looking for fundamentally better algorithms.  If you
still need speed, get out your assembler.
