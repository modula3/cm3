\section{xRoot: Roots of Functions}
\subsection{Quadratics}
\subsection*{quadreal, quadcmpx}
These address $ax^2+bx+c=0$ for real a,b,c and for complex a,b,c.
Internally, quadreal calls quadcmpx if the discriminant is less than zero.

We follow the analysis in NR92 pg 184.  However, we add generation of
alpha and beta terms via:
\begin{eqnarray}
  x_1    & = & \alpha + \beta\\
  x_2    & = & \alpha - \beta \mbox{ therefore, }\\  
  \alpha & = & \frac{x_1+x_2}{2}\\
  \beta  & = & \frac{x_1-x_2}{2}
\end{eqnarray}

[HGG: Initially I couldn't find a way to extract $\alpha$ and
$\beta$ from the NR92 approach.  I even implemented quadreal in
EXTENDED reals (80-bit), to cover potential truncations.  Finally I went
back and looked for an extraction.  The problem of course was that
I was initially looking for $\alpha$ and $\beta$ as sources for
$x_1$ and $x_2$, not as results.]

\subsection{Nonlinear Functions}
In one dimension, root finding means finding a value x such
that $f(x) = 0$.  Generally you need a clue to get started,
e.g., from examining a graph.  Examination allows selection
of a bracketing pair, x1 and x2, which straddles the x-axis
(and thus straddles at least one root).  NR92 argues
persuasively that you should {\em always} bracket the root
before applying a numerical technique.

That brings up the next question.  How can you recognize a
bracket pair?  f(x1) will have the opposite sign from f(x2).
Here are some approaches:
\begin{verbatim}
     Given: y1:=f(x1); y2:=f(x2);
     
     a) IF (y1>0.0 AND y2<0.0) OR (y1<0.0 AND y2>0.0) THEN (*bracketed*)
     
     b) IF y1*y2<0.0 THEN (*bracketed*)
     
     c) IF sgn(y1) = -sgn(y2) THEN (*bracketed*)
\end{verbatim}

I haven't timed these out, but on the basis of short-circuit
evaulation of relationals, and on the basis of no procedure
calls and no multiplies, "a" should be the best.  I'll use
it in the following routines.

Given an arbitrary pair, we can reach out further and
further trying to get a bracket, or we can close the gap
narrower and narrower.  Both can be useful.

\subsection*{bracket\_out}
Inspired by NR92's zbrac.  The idea is to start with two
points, expand by the golden ratio iteratively, and see if
there comes a time when the y's are of opposite signs.

NR92 uses 1.6 as the growth factor.  Just to be different,
I'll use the Golden constant.  Also,
I'll require $x1<x2$.  NR92's algorithm actually works for
$x2<x1$ also, but it isn't as obvious.  I think the slight
reduction in generality is more than paid back in
readability.

\subsection*{bracket\_in}
Inspired by NR92's zbrak.  In testing, I noticed that when
the segments just happen to line up exactly on a root, they
miss it.  So doing different ranges is a good idea,
or doing n's which are not multiples of one another.  [HGG: All
things considered, I like this routine better than
bracket\_out, but they can work together.]

\subsection*{root\_bisect}
Inspired by NR92's rtbis.  Also covered by Hopk88, pg 67.

The basic trick is to note that the segment sizes get
smaller by 1/2 every time, so we can just do:
\begin{verbatim}
     h:=x2-x1;
     ...
     h:=0.5*h;
\end{verbatim}

The exit criterion is:
\begin{verbatim}
     IF h<tol THEN RETURN x; END;
\end{verbatim}

Each iteration requires a decision on where to go next,
based on y values.  But to do this, we need to know which
direction in x corresponds to which direction in y.  So we
need to orient the function.  NR92 does $f>0$ at $x+dx$.  Just
to be different, I'll do the opposite:
\begin{verbatim}
     y:=func(x1);
     IF y>0.0 THEN
       x:=x2; h:=x1-x2;
     ELSE
       x:=x1; h:=x2-x1;
     END;
     (*initialize*)
     h:=h*0.5; x:=x+h;
     FOR i:=1 TO maxiter DO
       y:=func(x);
       IF y<0 THEN
         x:=x+h;
       ELSE
         x:=x-h;
       END;
       IF h<xacc THEN RETURN x; END;
       h:=h*0.5;
     END;
\end{verbatim}
     
I decided to precalc y1 and y2 in order to do the bracketing
check.  That led to slight modifications in the
initialization.  Next, after running into cases of hitting
the root dead on, I decided to add a check for y=0.  I also
had a nice bug in the $h < {\mbox xacc}$ line.  I was vaguely thinking h
would always be positive, so I skipped doing an ABS(h) for
the comparison with xacc.  That of course failed, because if
func has a negative slope, h is negative.  Putting in ABS
did the job.


\subsection*{root\_brent}
Brent's algorithm is used in the netlib matrixh libraries.
The quadratic formula is given in NR92, eqn 9.3.1. \dots 9.3.5.
But NR92 only gives code, not the algorithm.  To demonstrate
derivation from the ideas rather than the raw code, I have
used more descriptive naming than NR92, and changed the use
of temporaries.  I also added quick victory checks at the
start.

The bracket pair $a \dots b$ is always the biggest interval of
interest.  Sometimes we also have a point c which is known
be on a's side but a little closer to b (thus giving a
smaller bracket).  At worst, c is identical to a.  So the
best shot for the next interval is $b-c$, called diffnext.

The hard part is deciding what to do about tolerances.  The
problem is that Brent's algorithm is carefully crafted with
truncation errors in mind.  You can't just go around pulling
temp variables out of loops etc. [HGG: Which I did at one point,
then thought better of it.]

After building the code, I tested with $x1>x2$, $x2>x1$, $x's <0$,
$x's > 0$, and x's straddling 0.  Does just fine as long as
there is only one root in the x1 \dots x2 range.  But I can get
$root=0.0$ or a genuine error if there are $>1$ roots.  I
suspect that is due to confusing the c as it moves back and
forth between a and b.  Maybe I'll look at this more later.


\subsection*{root\_newtraph}
The basic newton-raphson root finder is analyzed by Hopk88,
pg 83 and an algorithm is given in Krey88, pg 953.  NR92
also covers it, pg .  The formula is:
\begin{equation}    
  x_{i+1}:=x_i-f(x_i)/f'(x_i)
\end{equation}
Thus, you must be able to provide $f'(x)$.

The various authors agree that this algorithm can go wrong
sometimes.  NR92 addresses that by using Brent's idea of
dropping back to bisection in a pinch.  We will follow their
lead.  The question is, when should you go to bisection?
Given brackets a and b, and r for rootnext:
\begin{verbatim}
     a........r.........b
\end{verbatim}

Then $(a-r)*(r-b)$ should be positive no matrixter which
direction the axis points.  That is, it could be 2 positives
or 2 negatives.  If we get a negative from the multiply, r
is out of bounds.

When we need bisection, we can do: $\mbox{root}:=0.5*(a+b)$.
Again, we don't care whether $a<b$ or $a>b$.  However, to have a
delta for checking exit criteria, we need:
\begin{verbatim}
     tmp:=root;
     root:=0.5*(a+b);
     delta:=root-tmp;
\end{verbatim}

We start with an arbitrary r set at a, and work from there.
To assure that a and b remain bracketing, we need to stick
to a convention.  Let a be such that $f(a)<0$ and b such that
$f(b)>0$.  Then when a new root is formed, find its f, and set
the proper a or b to root.

NR92 also provides tests for converging too slowly.  My
algorithm just relies on bisection to safely drag the
solution to the root.  I have not found conditions which
make this an issue --- if some arise, we'll deal with it
then.
